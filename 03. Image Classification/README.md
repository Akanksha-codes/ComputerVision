# **Computer Vision: Image Classification**

The [**Simplepreprocessor**](https://github.com/ThinamXx/ComputerVision/blob/main/03.%20Image%20Classification/Simplepreprocessor.ipynb) notebook contains implementation of simple image preprocessor, loading an image dataset into memory, and K-Nearest Neighbor Classifier. K-Nearest Neighbor Classifier doesnâ€™t actually learn anything, but it directly relies on the distance between feature vectors.

The [**GradientDescent**](https://github.com/ThinamXx/ComputerVision/blob/main/03.%20Image%20Classification/GradientDescent.ipynb) notebook contains implementation of Gradient Descent Algorithms. 

The [**StochasticGradientDescent**](https://github.com/ThinamXx/ComputerVision/blob/main/03.%20Image%20Classification/StochasticGradientDescent.ipynb) notebook contains implementation of Stochastic Gradient Descent, Image Classification and Regularization. 

**Notebook:**
- ðŸ“‘[**Simplepreprocessor**](https://github.com/ThinamXx/ComputerVision/blob/main/03.%20Image%20Classification/Simplepreprocessor.ipynb)
- ðŸ“‘[**LinearClassifier**](https://github.com/ThinamXx/ComputerVision/blob/main/03.%20Image%20Classification/LinearClassifier.ipynb) 
- ðŸ“‘[**GradientDescent**](https://github.com/ThinamXx/ComputerVision/blob/main/03.%20Image%20Classification/GradientDescent.ipynb) 
- ðŸ“‘[**StochasticGradientDescent**](https://github.com/ThinamXx/ComputerVision/blob/main/03.%20Image%20Classification/StochasticGradientDescent.ipynb) 

**Linear Classifier**
- I have presented the notes about K-Nearest Neighbor and Parameterized Learning here in the snapshot.

![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%204.PNG)

**Gradient Descent**
- The gradient descent method is an iterative optimization algorithm that operates over a loss landscape also called and optimization surface. Also, gradient descent refers to the process of attempting to optimize the parameters for low loss and high classification accuracy via an iterative process of taking a step in the direction that minimize loss. I have presented the notes about Gradient Descent and Optimization here in the snapshot.

![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%205.PNG)

**Stochastic Gradient Descent**
- Stochastic Gradient Descent is a simple modification to the standard gradient descent algorithm that computes the gradient and updates the weight matrix on small batches of training data, rather than the entire training set. I have presented the implementation of Sigmoid Activation Function and Stochastic Gradient Descent and notes about Regularization here in the snapshots.

![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%207a.PNG)
![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%207b.PNG)

**Simple Preprocessor and Dataset Loader**
- I will build an image preprocessor that resizes the image, ignoring the aspect ratio. I have presented the implementation of Image Preprocessor and Dataset Loader here in the snapshot. 

![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%202.PNG)

**K-Nearest Neighbor**
- K-Nearest Neighbor Classifier doesnâ€™t actually learn anything, but it directly relies on the distance between feature vectors. I have presented the implementation of K-Nearest Neighbor Classifier and Model Evaluation here in the snapshot. 

![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%203.PNG)
