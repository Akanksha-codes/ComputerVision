{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientDescent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2KUe-1KL0gv"
      },
      "source": [
        "**INITIALIZATION:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52HHNInaIFXX"
      },
      "source": [
        "#@ IMPORTING NECESSARY PACKAGES AND LIBRARIES: \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsD1Ff5cJHI7"
      },
      "source": [
        "#@ DEFINING SIGMOID ACTIVATION FUNCTION: \n",
        "def sigmoid_activation(x):\n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "#@ DEFINING PREDICT FUNCTION:\n",
        "def predict(X, W):                                     # Defining Function. \n",
        "    preds = sigmoid_activation(X.dot(W))               # Initializing Dot Product. \n",
        "    preds[preds <= 0.5] = 0                            # Implementing Thresholds. \n",
        "    preds[preds > 0.5] = 1                             # Implementing Thresholds. \n",
        "    return preds                                       # Getting Predictions. "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwxpgnRRMYvq"
      },
      "source": [
        "**GETTING THE DATASET:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI5giYpSLxUY"
      },
      "source": [
        "#@ GETTING THE DATASET: \n",
        "(X, y) = make_blobs(n_samples=1000, n_features=2, centers=2, \n",
        "                    cluster_std=1.5, random_state=42)           # Initializing Classification Dataset. \n",
        "y = y.reshape((y.shape[0], 1))                                  # Reshaping Dataset. \n",
        "X = np.c_[X, np.ones((X.shape[0]))]                             # Inserting Column Matrix. \n",
        "(trainX, testX, trainY, testY) = train_test_split(\n",
        "    X, y, test_size=0.5, random_state=42)                       # Partitioning Dataset into Training and Testing. "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnsO4PoiQIA-"
      },
      "source": [
        "**GRADIENT DESCENT:**\n",
        "- The gradient descent method is an iterative optimization algorithm that operates over a loss landscape also called and optimization surface. Also, gradient descent refers to the process of attempting to optimize the parameters for low loss and high classification accuracy via an iterative process of taking a step in the direction that minimize loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ0BX-1NPITM",
        "outputId": "1b08abc6-29ef-4b93-f9f6-b03a6c98203b"
      },
      "source": [
        "#@ INITIALIZING PARAMETERS: \n",
        "print(\"[INFO] training...\")\n",
        "epochs, lr = 100, 0.01                                 # Initializing Epochs and LR. \n",
        "W = np.random.randn(X.shape[1], 1)                     # Initializing Weights. \n",
        "losses = []                                            # Initialization. \n",
        "\n",
        "#@ INITIALIZING GRADIENT DESCENT: \n",
        "for epoch in np.arange(0, epochs):\n",
        "    preds = sigmoid_activation(trainX.dot(W))          # Initializing Sigmoid Activation.\n",
        "    error = preds - trainY                             # Computing Error. \n",
        "    loss = np.sum(error**2)                            # Getting Loss. \n",
        "    losses.append(loss)\n",
        "    gradient = trainX.T.dot(error)                     # Computing Gradient Descent. \n",
        "    W -= lr * gradient                                 # Updating Weight Matrices. \n",
        "    if epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "        print(\"[INFO] epoch={}, loss={:.7f}\".format(\n",
        "            int(epoch+1), loss))                       # Inspecting Updates. "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training...\n",
            "[INFO] epoch=1, loss=9.9582837\n",
            "[INFO] epoch=5, loss=1.9492182\n",
            "[INFO] epoch=10, loss=1.2637659\n",
            "[INFO] epoch=15, loss=0.7558308\n",
            "[INFO] epoch=20, loss=0.4611423\n",
            "[INFO] epoch=25, loss=0.3268099\n",
            "[INFO] epoch=30, loss=0.2840683\n",
            "[INFO] epoch=35, loss=0.2715303\n",
            "[INFO] epoch=40, loss=0.2656368\n",
            "[INFO] epoch=45, loss=0.2612658\n",
            "[INFO] epoch=50, loss=0.2573203\n",
            "[INFO] epoch=55, loss=0.2535368\n",
            "[INFO] epoch=60, loss=0.2498476\n",
            "[INFO] epoch=65, loss=0.2462340\n",
            "[INFO] epoch=70, loss=0.2426900\n",
            "[INFO] epoch=75, loss=0.2392131\n",
            "[INFO] epoch=80, loss=0.2358015\n",
            "[INFO] epoch=85, loss=0.2324537\n",
            "[INFO] epoch=90, loss=0.2291683\n",
            "[INFO] epoch=95, loss=0.2259441\n",
            "[INFO] epoch=100, loss=0.2227798\n"
          ]
        }
      ]
    }
  ]
}