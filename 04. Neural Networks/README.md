# **Computer Vision: Neural Networks**

The [**Neural Networks**](https://github.com/ThinamXx/ComputerVision/blob/main/04.%20Neural%20Networks/NeuralNetworks.ipynb) notebook contains the implementation of **Perceptron** algorithm, backpropagation algorithm, and neural networks from scratch. 

ðŸ“š[**Notebooks:**]
- [**Neural Network**](https://github.com/ThinamXx/ComputerVision/blob/main/04.%20Neural%20Networks/NeuralNetworks.ipynb)

**Rectified Linear Unit**
- ReLU is zero for negative inputs but increases linearly for positive inputs. The ReLU function is not saturable and is also extremely computationally efficient. ReLU is the most popular activation function used in deep learning and has stronger biological motivations.

**Perceptron Algorithm**
- I have presented the implementation of Perceptron Algorithm here in the snapshot.

![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%2011.PNG) 

**Backpropagation Algorithm**
-  I have presented the implementation of Neural Network and Backpropagation here in the snapshot.

![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%2012a.PNG)
![Image](https://github.com/ThinamXx/MachineLearning_DeepLearning/blob/main/Images/Day%2012b.PNG)
