{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INITIALIZATION:**"
      ],
      "metadata": {
        "id": "4kMIRQvmkig0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AlCfmSxgNqhr"
      },
      "outputs": [],
      "source": [
        "#@ INITIALIZING NECESSARY PACKAGES AND DEPENDENCIES: \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERCEPTRON:**"
      ],
      "metadata": {
        "id": "2hpm7_bOkc1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING PERCEPTRON MODEL: \n",
        "class Perceptron:                                       # Defining Perceptron. \n",
        "    def __init__(self, N, alpha=0.1):                   # Initializing Constructor Function. \n",
        "        self.W = np.random.randn(N + 1) / np.sqrt(N)    # Initializing Scaled Weight Matrix. \n",
        "        self.alpha = alpha                              # Initializing LR.\n",
        "    \n",
        "    def step(self, x):                                  # Defining Step Function. \n",
        "        return 1 if x > 0 else 0                        # Getting 1 if Positive else Negative. \n",
        "    \n",
        "    def fit(self, X, y, epochs=10):                     # Defining Fit Function. \n",
        "        X = np.c_[X, np.ones((X.shape[0]))]             # Adding Column of Ones. \n",
        "        for epoch in np.arange(0, epochs):\n",
        "            for (x, target) in zip(X, y):\n",
        "                p = self.step(np.dot(x, self.W))        # Initializing Dot Product. \n",
        "                if p != target:\n",
        "                    error = p - target                  # Computing Error. \n",
        "                    self.W += -self.alpha*error*x       # Updating Weight Matrix. \n",
        "    \n",
        "    def predict(self, X, addBias=True):                 # Defining Predict Function. \n",
        "        X = np.atleast_2d(X)                            # Inspecting 2D Matrix. \n",
        "        if addBias:\n",
        "            X = np.c_[X, np.ones((X.shape[0]))]         # Adding Bias Vector. \n",
        "        return self.step(np.dot(X, self.W))             # Initializing Dot Product. "
      ],
      "metadata": {
        "id": "14phxR8kRMtB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ EVALUATING PERCEPTRON BITWISE DATASETS: OR:\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])          # Initializing Array Example. \n",
        "y = np.array([[0], [1], [1], [1]])                      # Initializing Array Example. \n",
        "\n",
        "#@ TRAINING PERCEPTRON MODEL: \n",
        "p = Perceptron(X.shape[1], alpha=0.1)                   # Initializing Perceptron Model. \n",
        "p.fit(X, y, epochs=20)                                  # Training Model. \n",
        "\n",
        "#@ MODEL EVALUATION: \n",
        "for (x, target) in zip(X, y):\n",
        "    pred = p.predict(x)\n",
        "    print(f\"data={x}, ground-truth={target}, pred={pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqHQfynQb_G_",
        "outputId": "6d2617a5-8f0d-438a-bceb-672c5bfd748f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data=[0 0], ground-truth=[0], pred=0\n",
            "data=[0 1], ground-truth=[1], pred=1\n",
            "data=[1 0], ground-truth=[1], pred=1\n",
            "data=[1 1], ground-truth=[1], pred=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEURAL NETWORKS:**"
      ],
      "metadata": {
        "id": "vCm97EO6rjta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING BACKPROPAGATION ALGORITHM:\n",
        "class NeuralNetwork:                                            # Defining Neural Network. \n",
        "    def __init__(self, layers, alpha=0.1):                      # Initializing Constructor Function. \n",
        "        self.W = []                                             # Initialization. \n",
        "        self.layers = layers                                    # Initialization. \n",
        "        self.alpha = alpha                                      # Initialization. \n",
        "        for i in np.arange(0, len(layers) - 2):\n",
        "            w = np.random.randn(layers[i]+1, layers[i+1]+1)     # Initializing Weight Matrix. \n",
        "            self.W.append(w / np.sqrt(layers[i]))               # Normalizing Variance. \n",
        "        w = np.random.randn(layers[-2]+1, layers[-1])           # Initializing Weight Matrix. \n",
        "        self.W.append(w / np.sqrt(layers[-2]))                  # Normalizing Variance. \n",
        "    \n",
        "    def __repr__(self):                                         # Function for Debugging. \n",
        "        return \"NeuralNetwork: {}\".format(\n",
        "            \"-\".join(str(l) for l in self.layers))              # Inspecting. \n",
        "    \n",
        "    def sigmoid(self, x):                                       # Defining Sigmoid Function. \n",
        "        return 1.0 / (1 + np.exp(-x))                           # Implementation of Sigmoid Function. \n",
        "    \n",
        "    def sigmoid_deriv(self, x):                                 # Defining Function. \n",
        "        return x * (1 - x)                                      # Getting Derivative of Sigmoid. \n",
        "    \n",
        "    def fit(self, X, y, epochs=1000, displayUpdate=100):        # Defining Fit Function. \n",
        "        X = np.c_[X, np.ones((X.shape[0]))]                     # Adding Column of Ones. \n",
        "        for epoch in np.arange(0, epochs):\n",
        "            for (x, target) in zip(X, y):\n",
        "                self.fit_partial(x, target)                     # Backpropagation and Updating Matrix. \n",
        "            if epoch==0 or (epoch+1)%displayUpdate==0:\n",
        "                loss = self.calculate_loss(X, y)                # Initializing Loss Calculation. \n",
        "                print(\"epoch={}, loss={:.7f}\".format(\n",
        "                    epoch+1, loss))                             # Inspecting Loss. \n",
        "    \n",
        "    def fit_partial(self, x, y):                                # Defining Fit Partial Method. \n",
        "        A = [np.atleast_2d(x)]                                  # Initializing List. \n",
        "\n",
        "        #@ FEEDFORWARD:\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            net = A[layer].dot(self.W[layer])                   # Dot Product of Activation and Weight Matrix. \n",
        "            out = self.sigmoid(net)                             # Implementing Nonlinear Activation Function. \n",
        "            A.append(out)                                       # Adding to Activations. \n",
        "\n",
        "        #@ BACKPROPAGATION:\n",
        "        error = A[-1] - y                                       # Computing Error. \n",
        "        D = [error * self.sigmoid_deriv(A[-1])]                 # Initializing List of Deltas. \n",
        "        for layer in np.arange(len(A) - 2, 0, -1):\n",
        "            delta = D[-1].dot(self.W[layer].T)                  # Initializing Dot Product. \n",
        "            delta = delta * self.sigmoid_deriv(A[layer])        # Implementating Derivative Function. \n",
        "            D.append(delta)                                     # Adding Activations. \n",
        "        D = D[::-1]                                             # Reversing Activations. \n",
        "\n",
        "        #@ WEIGHT UPDATE PHASE: \n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
        "        \n",
        "    def predict(self, X, addBias=True):                         # Defining Predict Function. \n",
        "        p = np.atleast_2d(X)                                    # Initialization. \n",
        "        if addBias:\n",
        "            p = np.c_[p, np.ones((p.shape[0]))]                 # Adding Column of Ones. \n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            p = self.sigmoid(np.dot(p, self.W[layer]))          # Implementing Dot Product. \n",
        "        return p\n",
        "    \n",
        "    def calculate_loss(self, X, targets):                       # Defining Function. \n",
        "        targets = np.atleast_2d(targets)                        # Initialization. \n",
        "        predictions = self.predict(X, addBias=False)            # Implementation of Predict Method. \n",
        "        loss = 0.5 * np.sum((predictions - targets)**2)         # Computing Loss. \n",
        "        return loss"
      ],
      "metadata": {
        "id": "wgKuObXhri8_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ EVALUATING NEURAL NETWORKS ON BITWISE DATASETS: XOR:\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])                  # Initializing Array Example. \n",
        "y = np.array([[0], [1], [1], [0]])                              # Initializing Array Example. \n",
        "\n",
        "#@ TRAINING NEURAL NETWORKS MODEL: \n",
        "nn = NeuralNetwork([2, 2, 1], alpha=0.5)                        # Initializing Neural Network.  \n",
        "nn.fit(X, y, epochs=200)                                        # Training Model. \n",
        "\n",
        "#@ MODEL EVALUATION: \n",
        "for (x, target) in zip(X, y):\n",
        "    pred = nn.predict(x)[0][0]\n",
        "    step = 1 if pred > 0.5 else 0\n",
        "    print(f\"data={x}, ground-truth={target[0]}, pred={pred},\"\n",
        "          f\"step={step}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elrPa9oaNVr7",
        "outputId": "725e5afe-6434-4521-e3b2-144cdc7448e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=1, loss=0.5334357\n",
            "epoch=100, loss=0.4956250\n",
            "epoch=200, loss=0.4825043\n",
            "data=[0 0], ground-truth=0, pred=0.5128586749952443,step=1\n",
            "data=[0 1], ground-truth=1, pred=0.580395169002964,step=1\n",
            "data=[1 0], ground-truth=1, pred=0.47043960176292915,step=0\n",
            "data=[1 1], ground-truth=0, pred=0.49546158437567134,step=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fN8n83WbP1G_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}