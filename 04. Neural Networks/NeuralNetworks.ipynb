{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetworks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INITIALIZATION:**"
      ],
      "metadata": {
        "id": "4kMIRQvmkig0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AlCfmSxgNqhr"
      },
      "outputs": [],
      "source": [
        "#@ INITIALIZING NECESSARY PACKAGES AND DEPENDENCIES: \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import datasets\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers.core import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PERCEPTRON:**"
      ],
      "metadata": {
        "id": "2hpm7_bOkc1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING PERCEPTRON MODEL: \n",
        "class Perceptron:                                       # Defining Perceptron. \n",
        "    def __init__(self, N, alpha=0.1):                   # Initializing Constructor Function. \n",
        "        self.W = np.random.randn(N + 1) / np.sqrt(N)    # Initializing Scaled Weight Matrix. \n",
        "        self.alpha = alpha                              # Initializing LR.\n",
        "    \n",
        "    def step(self, x):                                  # Defining Step Function. \n",
        "        return 1 if x > 0 else 0                        # Getting 1 if Positive else Negative. \n",
        "    \n",
        "    def fit(self, X, y, epochs=10):                     # Defining Fit Function. \n",
        "        X = np.c_[X, np.ones((X.shape[0]))]             # Adding Column of Ones. \n",
        "        for epoch in np.arange(0, epochs):\n",
        "            for (x, target) in zip(X, y):\n",
        "                p = self.step(np.dot(x, self.W))        # Initializing Dot Product. \n",
        "                if p != target:\n",
        "                    error = p - target                  # Computing Error. \n",
        "                    self.W += -self.alpha*error*x       # Updating Weight Matrix. \n",
        "    \n",
        "    def predict(self, X, addBias=True):                 # Defining Predict Function. \n",
        "        X = np.atleast_2d(X)                            # Inspecting 2D Matrix. \n",
        "        if addBias:\n",
        "            X = np.c_[X, np.ones((X.shape[0]))]         # Adding Bias Vector. \n",
        "        return self.step(np.dot(X, self.W))             # Initializing Dot Product. "
      ],
      "metadata": {
        "id": "14phxR8kRMtB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ EVALUATING PERCEPTRON BITWISE DATASETS: OR:\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])          # Initializing Array Example. \n",
        "y = np.array([[0], [1], [1], [1]])                      # Initializing Array Example. \n",
        "\n",
        "#@ TRAINING PERCEPTRON MODEL: \n",
        "p = Perceptron(X.shape[1], alpha=0.1)                   # Initializing Perceptron Model. \n",
        "p.fit(X, y, epochs=20)                                  # Training Model. \n",
        "\n",
        "#@ MODEL EVALUATION: \n",
        "for (x, target) in zip(X, y):\n",
        "    pred = p.predict(x)\n",
        "    print(f\"data={x}, ground-truth={target}, pred={pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqHQfynQb_G_",
        "outputId": "ed60aac0-1454-4ba7-f011-601008ae1a8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data=[0 0], ground-truth=[0], pred=0\n",
            "data=[0 1], ground-truth=[1], pred=1\n",
            "data=[1 0], ground-truth=[1], pred=1\n",
            "data=[1 1], ground-truth=[1], pred=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEURAL NETWORKS: BACKPROPAGATION**"
      ],
      "metadata": {
        "id": "vCm97EO6rjta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ INITIALIZING BACKPROPAGATION ALGORITHM:\n",
        "class NeuralNetwork:                                            # Defining Neural Network. \n",
        "    def __init__(self, layers, alpha=0.1):                      # Initializing Constructor Function. \n",
        "        self.W = []                                             # Initialization. \n",
        "        self.layers = layers                                    # Initialization. \n",
        "        self.alpha = alpha                                      # Initialization. \n",
        "        for i in np.arange(0, len(layers) - 2):\n",
        "            w = np.random.randn(layers[i]+1, layers[i+1]+1)     # Initializing Weight Matrix. \n",
        "            self.W.append(w / np.sqrt(layers[i]))               # Normalizing Variance. \n",
        "        w = np.random.randn(layers[-2]+1, layers[-1])           # Initializing Weight Matrix. \n",
        "        self.W.append(w / np.sqrt(layers[-2]))                  # Normalizing Variance. \n",
        "    \n",
        "    def __repr__(self):                                         # Function for Debugging. \n",
        "        return \"NeuralNetwork: {}\".format(\n",
        "            \"-\".join(str(l) for l in self.layers))              # Inspecting. \n",
        "    \n",
        "    def sigmoid(self, x):                                       # Defining Sigmoid Function. \n",
        "        return 1.0 / (1 + np.exp(-x))                           # Implementation of Sigmoid Function. \n",
        "    \n",
        "    def sigmoid_deriv(self, x):                                 # Defining Function. \n",
        "        return x * (1 - x)                                      # Getting Derivative of Sigmoid. \n",
        "    \n",
        "    def fit(self, X, y, epochs=1000, displayUpdate=100):        # Defining Fit Function. \n",
        "        X = np.c_[X, np.ones((X.shape[0]))]                     # Adding Column of Ones. \n",
        "        for epoch in np.arange(0, epochs):\n",
        "            for (x, target) in zip(X, y):\n",
        "                self.fit_partial(x, target)                     # Backpropagation and Updating Matrix. \n",
        "            if epoch==0 or (epoch+1)%displayUpdate==0:\n",
        "                loss = self.calculate_loss(X, y)                # Initializing Loss Calculation. \n",
        "                print(\"epoch={}, loss={:.7f}\".format(\n",
        "                    epoch+1, loss))                             # Inspecting Loss. \n",
        "    \n",
        "    def fit_partial(self, x, y):                                # Defining Fit Partial Method. \n",
        "        A = [np.atleast_2d(x)]                                  # Initializing List. \n",
        "\n",
        "        #@ FEEDFORWARD:\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            net = A[layer].dot(self.W[layer])                   # Dot Product of Activation and Weight Matrix. \n",
        "            out = self.sigmoid(net)                             # Implementing Nonlinear Activation Function. \n",
        "            A.append(out)                                       # Adding to Activations. \n",
        "\n",
        "        #@ BACKPROPAGATION:\n",
        "        error = A[-1] - y                                       # Computing Error. \n",
        "        D = [error * self.sigmoid_deriv(A[-1])]                 # Initializing List of Deltas. \n",
        "        for layer in np.arange(len(A) - 2, 0, -1):\n",
        "            delta = D[-1].dot(self.W[layer].T)                  # Initializing Dot Product. \n",
        "            delta = delta * self.sigmoid_deriv(A[layer])        # Implementating Derivative Function. \n",
        "            D.append(delta)                                     # Adding Activations. \n",
        "        D = D[::-1]                                             # Reversing Activations. \n",
        "\n",
        "        #@ WEIGHT UPDATE PHASE: \n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
        "        \n",
        "    def predict(self, X, addBias=True):                         # Defining Predict Function. \n",
        "        p = np.atleast_2d(X)                                    # Initialization. \n",
        "        if addBias:\n",
        "            p = np.c_[p, np.ones((p.shape[0]))]                 # Adding Column of Ones. \n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            p = self.sigmoid(np.dot(p, self.W[layer]))          # Implementing Dot Product. \n",
        "        return p\n",
        "    \n",
        "    def calculate_loss(self, X, targets):                       # Defining Function. \n",
        "        targets = np.atleast_2d(targets)                        # Initialization. \n",
        "        predictions = self.predict(X, addBias=False)            # Implementation of Predict Method. \n",
        "        loss = 0.5 * np.sum((predictions - targets)**2)         # Computing Loss. \n",
        "        return loss"
      ],
      "metadata": {
        "id": "wgKuObXhri8_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ EVALUATING NEURAL NETWORKS ON BITWISE DATASETS: XOR:\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])                  # Initializing Array Example. \n",
        "y = np.array([[0], [1], [1], [0]])                              # Initializing Array Example. \n",
        "\n",
        "#@ TRAINING NEURAL NETWORKS MODEL: \n",
        "nn = NeuralNetwork([2, 2, 1], alpha=0.5)                        # Initializing Neural Network.  \n",
        "nn.fit(X, y, epochs=200)                                        # Training Model. \n",
        "\n",
        "#@ MODEL EVALUATION: \n",
        "for (x, target) in zip(X, y):\n",
        "    pred = nn.predict(x)[0][0]\n",
        "    step = 1 if pred > 0.5 else 0\n",
        "    print(f\"data={x}, ground-truth={target[0]}, pred={pred},\"\n",
        "          f\"step={step}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elrPa9oaNVr7",
        "outputId": "55262809-d850-45d0-efe9-3d774a74c618"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=1, loss=0.5003620\n",
            "epoch=100, loss=0.4991073\n",
            "epoch=200, loss=0.4973769\n",
            "data=[0 0], ground-truth=0, pred=0.4975027342572825,step=0\n",
            "data=[0 1], ground-truth=1, pred=0.5119170447281081,step=1\n",
            "data=[1 0], ground-truth=1, pred=0.503353226845293,step=1\n",
            "data=[1 1], ground-truth=0, pred=0.5122126284343791,step=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MNIST SAMPLE:**"
      ],
      "metadata": {
        "id": "oeV74YDzh1PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GETTING THE DATASET:\n",
        "digits = datasets.load_digits()                         # Initializing Dataset. \n",
        "data = digits.data.astype(\"float\")                      # Converting into Float. \n",
        "data = (data - data.min()) / (data.max() - data.min())  # Normalizing Dataset. \n",
        "print(\"samples: {}, dim: {}\".format(data.shape[0],\n",
        "                                  data.shape[1]))       # Inspecting Dataset. "
      ],
      "metadata": {
        "id": "fN8n83WbP1G_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a62eec-38fe-48e8-af61-5b803a8a81ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "samples: 1797, dim: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@ PREPARING THE DATASET:\n",
        "(train_X, test_X, train_Y, test_Y) = train_test_split(\n",
        "    data, digits.target, test_size=0.25)                    # Splitting Dataset. \n",
        "train_Y = LabelBinarizer().fit_transform(train_Y)           # Converting Labels to Vectors. \n",
        "test_Y = LabelBinarizer().fit_transform(test_Y)             # Converting Labels to Vectors. \n",
        "\n",
        "#@ TRAINING THE NEURAL NETWORK:\n",
        "nn = NeuralNetwork([train_X.shape[1], 32, 16, 10])          # Initializing Neural Network. \n",
        "nn.fit(train_X, train_Y, epochs=1000)                       # Training Neural Network.\n",
        " \n",
        "#@ EVALUATING THE NEURAL NETWORK: \n",
        "predictions = nn.predict(test_X)                            # Getting Predictions. \n",
        "predictions = predictions.argmax(axis=1)                    # Getting Maximum Probability. \n",
        "print(classification_report(\n",
        "    test_Y.argmax(axis=1), predictions))                    # Inspecting Classification Report. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGX1TqUvj6vy",
        "outputId": "f7cf8b0b-3414-46e5-b9ce-f855c19eec1c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=1, loss=605.7806718\n",
            "epoch=100, loss=5.7756296\n",
            "epoch=200, loss=2.9896694\n",
            "epoch=300, loss=2.5347345\n",
            "epoch=400, loss=2.3556062\n",
            "epoch=500, loss=2.0657081\n",
            "epoch=600, loss=1.2556464\n",
            "epoch=700, loss=1.1949743\n",
            "epoch=800, loss=1.1591630\n",
            "epoch=900, loss=1.1346962\n",
            "epoch=1000, loss=1.1167485\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96        52\n",
            "           1       0.96      1.00      0.98        48\n",
            "           2       1.00      0.95      0.97        40\n",
            "           3       0.98      0.93      0.96        46\n",
            "           4       0.95      1.00      0.97        38\n",
            "           5       0.97      0.88      0.93        42\n",
            "           6       0.96      1.00      0.98        52\n",
            "           7       0.96      0.96      0.96        45\n",
            "           8       0.97      0.97      0.97        40\n",
            "           9       0.88      0.96      0.92        47\n",
            "\n",
            "    accuracy                           0.96       450\n",
            "   macro avg       0.96      0.96      0.96       450\n",
            "weighted avg       0.96      0.96      0.96       450\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEURAL NETWORKS:**"
      ],
      "metadata": {
        "id": "Z_QhpG32tp6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@ GETTING THE DATASET: \n",
        "dataset = datasets.fetch_openml(\"mnist_784\")                # Downloading Dataset. \n",
        "data = dataset.data.astype(\"float\") / 255.0                 # Initializing Data Normalization. \n",
        "\n",
        "#@ PREPARING THE DATASET:\n",
        "(train_X, test_X, train_Y, test_Y) = train_test_split(\n",
        "    data, dataset.target, test_size=0.25)                   # Splitting Dataset. \n",
        "train_Y = LabelBinarizer().fit_transform(train_Y)           # Converting Labels to Vectors. \n",
        "test_Y = LabelBinarizer().fit_transform(test_Y)             # Converting Labels to Vectors. "
      ],
      "metadata": {
        "id": "YPSRJZIUs_nU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ DEFINING NETWORK ARCHITECTURE: \n",
        "model = Sequential()                                            # Initializing Sequential Model. \n",
        "model.add(Dense(256, input_shape=(784,), activation=\"sigmoid\")) # Initializing Dense Layer. \n",
        "model.add(Dense(128, activation=\"sigmoid\"))                     # Adding Dense Layer. \n",
        "model.add(Dense(10, activation=\"softmax\"))                      # Adding Dense Output Layer. \n",
        "\n",
        "#@ TRAINING NEURAL NETWORK:\n",
        "sgd = SGD(0.01)                                                 # Initializing SGD Optimizer. \n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, \n",
        "              metrics=[\"accuracy\"])                             # Compiling Model. \n",
        "H = model.fit(train_X, train_Y,validation_data=(test_X,test_Y), \n",
        "              epochs=100, batch_size=128)                       # Training Model. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMQacNBtv6Pr",
        "outputId": "374a5b10-26ba-42d9-87b6-a38fe9cf47c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "411/411 [==============================] - 5s 9ms/step - loss: 2.2734 - accuracy: 0.2027 - val_loss: 2.2424 - val_accuracy: 0.3757\n",
            "Epoch 2/100\n",
            "411/411 [==============================] - 4s 9ms/step - loss: 2.2117 - accuracy: 0.3997 - val_loss: 2.1809 - val_accuracy: 0.5271\n",
            "Epoch 3/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 2.1409 - accuracy: 0.5214 - val_loss: 2.0989 - val_accuracy: 0.5877\n",
            "Epoch 4/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 2.0442 - accuracy: 0.5869 - val_loss: 1.9851 - val_accuracy: 0.6066\n",
            "Epoch 5/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 1.9111 - accuracy: 0.6264 - val_loss: 1.8310 - val_accuracy: 0.6790\n",
            "Epoch 6/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 1.7409 - accuracy: 0.6637 - val_loss: 1.6460 - val_accuracy: 0.6683\n",
            "Epoch 7/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 1.5495 - accuracy: 0.6915 - val_loss: 1.4522 - val_accuracy: 0.7088\n",
            "Epoch 8/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 1.3634 - accuracy: 0.7217 - val_loss: 1.2761 - val_accuracy: 0.7103\n",
            "Epoch 9/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 1.2011 - accuracy: 0.7449 - val_loss: 1.1274 - val_accuracy: 0.7686\n",
            "Epoch 10/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 1.0682 - accuracy: 0.7703 - val_loss: 1.0083 - val_accuracy: 0.7795\n",
            "Epoch 11/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.9614 - accuracy: 0.7892 - val_loss: 0.9129 - val_accuracy: 0.7966\n",
            "Epoch 12/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.8753 - accuracy: 0.8039 - val_loss: 0.8367 - val_accuracy: 0.8119\n",
            "Epoch 13/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.8054 - accuracy: 0.8157 - val_loss: 0.7715 - val_accuracy: 0.8211\n",
            "Epoch 14/100\n",
            "411/411 [==============================] - 4s 9ms/step - loss: 0.7476 - accuracy: 0.8261 - val_loss: 0.7195 - val_accuracy: 0.8310\n",
            "Epoch 15/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.6994 - accuracy: 0.8346 - val_loss: 0.6761 - val_accuracy: 0.8374\n",
            "Epoch 16/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.6588 - accuracy: 0.8407 - val_loss: 0.6382 - val_accuracy: 0.8489\n",
            "Epoch 17/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.6243 - accuracy: 0.8473 - val_loss: 0.6062 - val_accuracy: 0.8513\n",
            "Epoch 18/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.5949 - accuracy: 0.8533 - val_loss: 0.5787 - val_accuracy: 0.8583\n",
            "Epoch 19/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.5694 - accuracy: 0.8575 - val_loss: 0.5565 - val_accuracy: 0.8613\n",
            "Epoch 20/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.5473 - accuracy: 0.8617 - val_loss: 0.5351 - val_accuracy: 0.8665\n",
            "Epoch 21/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.5278 - accuracy: 0.8655 - val_loss: 0.5182 - val_accuracy: 0.8672\n",
            "Epoch 22/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.5108 - accuracy: 0.8694 - val_loss: 0.5012 - val_accuracy: 0.8712\n",
            "Epoch 23/100\n",
            "411/411 [==============================] - 4s 10ms/step - loss: 0.4956 - accuracy: 0.8716 - val_loss: 0.4877 - val_accuracy: 0.8731\n",
            "Epoch 24/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.4819 - accuracy: 0.8747 - val_loss: 0.4752 - val_accuracy: 0.8753\n",
            "Epoch 25/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.4699 - accuracy: 0.8773 - val_loss: 0.4638 - val_accuracy: 0.8771\n",
            "Epoch 26/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.4588 - accuracy: 0.8789 - val_loss: 0.4535 - val_accuracy: 0.8795\n",
            "Epoch 27/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.4488 - accuracy: 0.8809 - val_loss: 0.4453 - val_accuracy: 0.8806\n",
            "Epoch 28/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.4399 - accuracy: 0.8822 - val_loss: 0.4361 - val_accuracy: 0.8824\n",
            "Epoch 29/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.4316 - accuracy: 0.8840 - val_loss: 0.4286 - val_accuracy: 0.8846\n",
            "Epoch 30/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.4240 - accuracy: 0.8861 - val_loss: 0.4219 - val_accuracy: 0.8843\n",
            "Epoch 31/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.4170 - accuracy: 0.8875 - val_loss: 0.4156 - val_accuracy: 0.8853\n",
            "Epoch 32/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.4106 - accuracy: 0.8887 - val_loss: 0.4095 - val_accuracy: 0.8874\n",
            "Epoch 33/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.4047 - accuracy: 0.8896 - val_loss: 0.4046 - val_accuracy: 0.8884\n",
            "Epoch 34/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3991 - accuracy: 0.8906 - val_loss: 0.3996 - val_accuracy: 0.8885\n",
            "Epoch 35/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3940 - accuracy: 0.8921 - val_loss: 0.3947 - val_accuracy: 0.8900\n",
            "Epoch 36/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3891 - accuracy: 0.8938 - val_loss: 0.3901 - val_accuracy: 0.8914\n",
            "Epoch 37/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3847 - accuracy: 0.8942 - val_loss: 0.3861 - val_accuracy: 0.8922\n",
            "Epoch 38/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3804 - accuracy: 0.8951 - val_loss: 0.3825 - val_accuracy: 0.8927\n",
            "Epoch 39/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3765 - accuracy: 0.8958 - val_loss: 0.3785 - val_accuracy: 0.8938\n",
            "Epoch 40/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3727 - accuracy: 0.8969 - val_loss: 0.3755 - val_accuracy: 0.8933\n",
            "Epoch 41/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3692 - accuracy: 0.8974 - val_loss: 0.3716 - val_accuracy: 0.8951\n",
            "Epoch 42/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3658 - accuracy: 0.8983 - val_loss: 0.3684 - val_accuracy: 0.8965\n",
            "Epoch 43/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3626 - accuracy: 0.8986 - val_loss: 0.3652 - val_accuracy: 0.8970\n",
            "Epoch 44/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3596 - accuracy: 0.8998 - val_loss: 0.3634 - val_accuracy: 0.8978\n",
            "Epoch 45/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3567 - accuracy: 0.9000 - val_loss: 0.3605 - val_accuracy: 0.8977\n",
            "Epoch 46/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3540 - accuracy: 0.9008 - val_loss: 0.3575 - val_accuracy: 0.8993\n",
            "Epoch 47/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3512 - accuracy: 0.9009 - val_loss: 0.3552 - val_accuracy: 0.8997\n",
            "Epoch 48/100\n",
            "411/411 [==============================] - 5s 11ms/step - loss: 0.3487 - accuracy: 0.9017 - val_loss: 0.3528 - val_accuracy: 0.8993\n",
            "Epoch 49/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3462 - accuracy: 0.9021 - val_loss: 0.3503 - val_accuracy: 0.9011\n",
            "Epoch 50/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3439 - accuracy: 0.9030 - val_loss: 0.3482 - val_accuracy: 0.9013\n",
            "Epoch 51/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3416 - accuracy: 0.9031 - val_loss: 0.3461 - val_accuracy: 0.9018\n",
            "Epoch 52/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3394 - accuracy: 0.9040 - val_loss: 0.3447 - val_accuracy: 0.9018\n",
            "Epoch 53/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3373 - accuracy: 0.9043 - val_loss: 0.3424 - val_accuracy: 0.9026\n",
            "Epoch 54/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3352 - accuracy: 0.9047 - val_loss: 0.3401 - val_accuracy: 0.9030\n",
            "Epoch 55/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3333 - accuracy: 0.9052 - val_loss: 0.3387 - val_accuracy: 0.9035\n",
            "Epoch 56/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3313 - accuracy: 0.9056 - val_loss: 0.3369 - val_accuracy: 0.9035\n",
            "Epoch 57/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3295 - accuracy: 0.9063 - val_loss: 0.3352 - val_accuracy: 0.9040\n",
            "Epoch 58/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3276 - accuracy: 0.9069 - val_loss: 0.3339 - val_accuracy: 0.9042\n",
            "Epoch 59/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3260 - accuracy: 0.9071 - val_loss: 0.3317 - val_accuracy: 0.9055\n",
            "Epoch 60/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3242 - accuracy: 0.9075 - val_loss: 0.3300 - val_accuracy: 0.9060\n",
            "Epoch 61/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3225 - accuracy: 0.9082 - val_loss: 0.3287 - val_accuracy: 0.9063\n",
            "Epoch 62/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3209 - accuracy: 0.9084 - val_loss: 0.3273 - val_accuracy: 0.9065\n",
            "Epoch 63/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3193 - accuracy: 0.9087 - val_loss: 0.3257 - val_accuracy: 0.9070\n",
            "Epoch 64/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3178 - accuracy: 0.9094 - val_loss: 0.3244 - val_accuracy: 0.9074\n",
            "Epoch 65/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3163 - accuracy: 0.9098 - val_loss: 0.3225 - val_accuracy: 0.9077\n",
            "Epoch 66/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3148 - accuracy: 0.9103 - val_loss: 0.3213 - val_accuracy: 0.9085\n",
            "Epoch 67/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3134 - accuracy: 0.9111 - val_loss: 0.3203 - val_accuracy: 0.9081\n",
            "Epoch 68/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3120 - accuracy: 0.9107 - val_loss: 0.3187 - val_accuracy: 0.9088\n",
            "Epoch 69/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.3106 - accuracy: 0.9112 - val_loss: 0.3174 - val_accuracy: 0.9089\n",
            "Epoch 70/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3092 - accuracy: 0.9116 - val_loss: 0.3161 - val_accuracy: 0.9093\n",
            "Epoch 71/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3079 - accuracy: 0.9120 - val_loss: 0.3159 - val_accuracy: 0.9088\n",
            "Epoch 72/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3066 - accuracy: 0.9123 - val_loss: 0.3144 - val_accuracy: 0.9090\n",
            "Epoch 73/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3053 - accuracy: 0.9128 - val_loss: 0.3125 - val_accuracy: 0.9099\n",
            "Epoch 74/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3040 - accuracy: 0.9130 - val_loss: 0.3116 - val_accuracy: 0.9095\n",
            "Epoch 75/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3029 - accuracy: 0.9133 - val_loss: 0.3104 - val_accuracy: 0.9104\n",
            "Epoch 76/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3016 - accuracy: 0.9138 - val_loss: 0.3090 - val_accuracy: 0.9103\n",
            "Epoch 77/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.3005 - accuracy: 0.9137 - val_loss: 0.3084 - val_accuracy: 0.9104\n",
            "Epoch 78/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2992 - accuracy: 0.9140 - val_loss: 0.3072 - val_accuracy: 0.9110\n",
            "Epoch 79/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2981 - accuracy: 0.9147 - val_loss: 0.3061 - val_accuracy: 0.9111\n",
            "Epoch 80/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2969 - accuracy: 0.9148 - val_loss: 0.3047 - val_accuracy: 0.9117\n",
            "Epoch 81/100\n",
            "411/411 [==============================] - 3s 7ms/step - loss: 0.2959 - accuracy: 0.9154 - val_loss: 0.3041 - val_accuracy: 0.9120\n",
            "Epoch 82/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2948 - accuracy: 0.9154 - val_loss: 0.3028 - val_accuracy: 0.9124\n",
            "Epoch 83/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2937 - accuracy: 0.9154 - val_loss: 0.3020 - val_accuracy: 0.9126\n",
            "Epoch 84/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2927 - accuracy: 0.9158 - val_loss: 0.3009 - val_accuracy: 0.9122\n",
            "Epoch 85/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2916 - accuracy: 0.9163 - val_loss: 0.2998 - val_accuracy: 0.9135\n",
            "Epoch 86/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2905 - accuracy: 0.9164 - val_loss: 0.2993 - val_accuracy: 0.9128\n",
            "Epoch 87/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2896 - accuracy: 0.9166 - val_loss: 0.2983 - val_accuracy: 0.9135\n",
            "Epoch 88/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2886 - accuracy: 0.9168 - val_loss: 0.2975 - val_accuracy: 0.9133\n",
            "Epoch 89/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2875 - accuracy: 0.9171 - val_loss: 0.2969 - val_accuracy: 0.9135\n",
            "Epoch 90/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2866 - accuracy: 0.9173 - val_loss: 0.2956 - val_accuracy: 0.9142\n",
            "Epoch 91/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2855 - accuracy: 0.9174 - val_loss: 0.2944 - val_accuracy: 0.9145\n",
            "Epoch 92/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2846 - accuracy: 0.9178 - val_loss: 0.2939 - val_accuracy: 0.9143\n",
            "Epoch 93/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2837 - accuracy: 0.9180 - val_loss: 0.2932 - val_accuracy: 0.9146\n",
            "Epoch 94/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2827 - accuracy: 0.9185 - val_loss: 0.2916 - val_accuracy: 0.9157\n",
            "Epoch 95/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2818 - accuracy: 0.9186 - val_loss: 0.2915 - val_accuracy: 0.9147\n",
            "Epoch 96/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2809 - accuracy: 0.9191 - val_loss: 0.2900 - val_accuracy: 0.9159\n",
            "Epoch 97/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2800 - accuracy: 0.9191 - val_loss: 0.2893 - val_accuracy: 0.9162\n",
            "Epoch 98/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2791 - accuracy: 0.9196 - val_loss: 0.2887 - val_accuracy: 0.9164\n",
            "Epoch 99/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2783 - accuracy: 0.9194 - val_loss: 0.2876 - val_accuracy: 0.9164\n",
            "Epoch 100/100\n",
            "411/411 [==============================] - 3s 8ms/step - loss: 0.2774 - accuracy: 0.9198 - val_loss: 0.2870 - val_accuracy: 0.9170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BjEIDydK5Vyx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}