{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Layers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWddcLZPcLoU"
      },
      "source": [
        "**INITIALIZATION:**\n",
        "- I use these three lines of code on top of my each notebooks because it will help to prevent any problems while reloading the same project. And the third line of code helps to make visualization within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It4XA2PkcBZo"
      },
      "source": [
        "#@ INITIALIZATION: \n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCoJ3nIxcSfk"
      },
      "source": [
        "**DOWNLOADING LIBRARIES AND DEPENDENCIES:**\n",
        "- I have downloaded all the libraries and dependencies required for the project in one particular cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tARh3Gx3cPNy"
      },
      "source": [
        "#@ DOWNLOADING LIBRARIES AND DEPENDENCIES: \n",
        "import cv2\n",
        "import imutils\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten, Dropout\n",
        "from tensorflow.keras.layers import Dense \n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XusQfVezcewl"
      },
      "source": [
        "**GETTING THE DATA:**\n",
        "- I will use subset of **CALTECH-101** dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65FwRa6brfTa"
      },
      "source": [
        "#@ DOWNLOADING THE DATASET: UNCOMMENT BELOW: \n",
        "# !tar -zxvf \"/content/drive/MyDrive/Data/101_ObjectCategories.tar.gz\" -C \"/content/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZokBKC10hEU"
      },
      "source": [
        "**STRIDED NET:**  \n",
        "The **StridedNet** has following important characteristics :    \n",
        "- It uses strided convolutions rather than pooling operations to reduce volume size. \n",
        "- The first **Convolutional** layer uses 7X7 filters but all other layers in the network use 3X3 filters. \n",
        "- The normal distribution algorithm is used to initialize all weights in the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHMU2OX1y9ze"
      },
      "source": [
        "#@ INITIALIZATION OF STRIDEDNET: \n",
        "class StridedNet:                                                  # Defining StridedNet Class. \n",
        "    @staticmethod\n",
        "    def build(width,height,depth,classes,reg,init=\"he_normal\"):    # Defining Build Method. \n",
        "        model = Sequential()                                       # Initializing Sequential Model. \n",
        "        inputShape = (height, width, depth)                        # Initializing Input Shape. \n",
        "        chanDim = -1                                               # Initializing Channel. \n",
        "        if K.image_data_format() == \"channels_first\":              # Inspecting Data Format. \n",
        "            inputShape = (depth, height, width)                    # Initializing Input Shape. \n",
        "            chanDim = 1                                            # Initializing Channel. \n",
        "        model.add(Conv2D(16,(7,7),strides=(2,2),padding=\"valid\",\n",
        "                         kernel_initializer=init,\n",
        "                         kernel_regularizer=reg,\n",
        "                         input_shape=inputShape))                  # Initializing Convolutional Layer. \n",
        "        model.add(Conv2D(32, (3,3), padding=\"same\", \n",
        "                         kernel_initializer=init,\n",
        "                         kernel_regularizer=reg))                  # Initializing Convolutional Layer. \n",
        "        model.add(Activation(\"relu\"))                              # Initializing Activation Function. \n",
        "        model.add(BatchNormalization(axis=chanDim))                # Initializing BatchNormalization Layer. \n",
        "        model.add(Conv2D(32, (3,3), strides=(2,2), padding=\"same\",\n",
        "                         kernel_initializer=init, \n",
        "                         kernel_regularizer=reg, \n",
        "                         activation=\"relu\"))                       # Initializing Convolutional Layer. \n",
        "        model.add(BatchNormalization(axis=chanDim))                # Initializing BatchNormalization Layer.\n",
        "        model.add(Dropout(0.25))                                   # Initializing Dropout Layer. \n",
        "        model.add(Conv2D(64, (3,3), padding=\"same\", \n",
        "                         kernel_initializer=init, \n",
        "                         kernel_regularizer=reg))                  # Initializing Convolutional Layer. \n",
        "        model.add(Activation(\"relu\"))                              # Initializing Activation Function.  \n",
        "        model.add(BatchNormalization(axis=chanDim))                # Initializing BatchNormalization Layer.\n",
        "        model.add(Conv2D(64, (3,3), strides=(2,2), padding=\"same\", \n",
        "                         kernel_initializer=init, \n",
        "                         kernel_regularizer=reg))                  # Initializing Convolutional Layer. \n",
        "        model.add(Activation(\"relu\"))                              # Initializing Activation Function.\n",
        "        model.add(BatchNormalization(axis=chanDim))                # Initializing BatchNormalization Layer.\n",
        "        model.add(Dropout(0.25))                                   # Initializing Dropout Layer.\n",
        "        model.add(Conv2D(128, (3,3), padding=\"same\", \n",
        "                         kernel_initializer=init, \n",
        "                         kernel_regularizer=reg))                  # Initializing Convolutional Layer. \n",
        "        model.add(Activation(\"relu\"))                              # Initializing Activation Function.  \n",
        "        model.add(BatchNormalization(axis=chanDim))                # Initializing BatchNormalization Layer.\n",
        "        model.add(Conv2D(128,(3,3), strides=(2,2), padding=\"same\", \n",
        "                         kernel_initializer=init, \n",
        "                         kernel_regularizer=reg))                  # Initializing Convolutional Layer. \n",
        "        model.add(Activation(\"relu\"))                              # Initializing Activation Function.\n",
        "        model.add(BatchNormalization(axis=chanDim))                # Initializing BatchNormalization Layer.\n",
        "        model.add(Dropout(0.25))                                   # Initializing Dropout Layer.\n",
        "        model.add(Flatten())                                       # Initializing Fully Connected Layer. \n",
        "        model.add(Dense(512, kernel_initializer=init))             # Initializing Dense Layer. \n",
        "        model.add(Activation(\"relu\"))                              # Initializing Activation Function. \n",
        "        model.add(BatchNormalization(axis=chanDim))                # Initializing BatchNormalization Layer.\n",
        "        model.add(Dropout(0.5))                                    # Initializing Dropout Layer.\n",
        "        model.add(Dense(classes, activation=\"softmax\"))            # Initializing Dense Layer. \n",
        "        return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaC43j2KXgqA"
      },
      "source": [
        "**PREPARING THE DATASET:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDK1a1jj_4Gh"
      },
      "source": [
        "#@ PREPARING THE DATASET: \n",
        "PATH = \"/content/101_ObjectCategories\"                             # Path to Dataset. \n",
        "LABELS = set([\"Faces\", \"Leopards\", \"Motorbikes\", \"airplanes\"])     # Initializing Labels. \n",
        "imagePaths = list(paths.list_images(PATH))\n",
        "data, labels = [], []                                              # Initializing List. \n",
        "\n",
        "#@ PREPARING THE DATASET: \n",
        "for imagePath in imagePaths:\n",
        "    label = imagePath.split(os.path.sep)[-2]                       # Extracting Labels. \n",
        "    if label not in LABELS: \n",
        "        continue\n",
        "    image = cv2.imread(imagePath)                                  # Reading Image. \n",
        "    image = cv2.resize(image, (96, 96))                            # Resizing Image. \n",
        "    data.append(image)                                             # Updating Data. \n",
        "    labels.append(label)                                           # Updating Labels. "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo-EiZY3xLSe"
      },
      "source": [
        "#@ PREPARING THE DATASET: \n",
        "data = np.array(data, dtype=\"float\") / 255.0                       # Converting into Array and Scaling. \n",
        "lb = LabelBinarizer()                                              # Initializing Label Binarizer. \n",
        "labels = lb.fit_transform(labels)                                  # Initializing One Hot Encoding. \n",
        "(trainX, testX, trainY, testY) = train_test_split(\n",
        "    data,labels,test_size=0.25,stratify=labels,random_state=42)    # Initializing Training and Test Dataset. \n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, \n",
        "                         width_shift_range=0.2,\n",
        "                         height_shift_range=0.2,shear_range=0.15, \n",
        "                         horizontal_flip=True,fill_mode=\"nearest\") # Initializing Data Augmentation. "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39AQaVj5aUcu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}